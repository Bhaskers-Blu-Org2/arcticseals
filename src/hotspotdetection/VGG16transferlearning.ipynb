{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.optimizers import SGD\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Flatten\n",
    "from keras import backend as K\n",
    "from keras.utils import Sequence\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "imgPath = \"/mnt/ml/NOAA/color\"\n",
    "imgCropPath = \"/mnt/ml/NOAA/crop\"\n",
    "irPath = \"/mnt/ml/NOAA/thermal\"\n",
    "dataPath = \"/mnt/ml/NOAA/arcticseals/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metaparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_crop(img, x1, y1, x2, y2):\n",
    "    return img[y1:y2, x1:x2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5299 Validation: 1325\n",
      "(5299,)\n",
      "(1325,)\n"
     ]
    }
   ],
   "source": [
    "def GetPartitionsAndLabels():\n",
    "    df = pd.read_csv(\"{}/training.csv\".format(dataPath))\n",
    "    rowcount = int(df.shape[0])\n",
    "    train_count = int(rowcount * (1 - VALIDATION_SPLIT))\n",
    "    test_count = rowcount - train_count\n",
    "    print (\"Train: {} Validation: {}\".format(train_count, test_count))\n",
    "    \n",
    "    ids = df[\"hotspot_id\"].astype(str).values\n",
    "   \n",
    "    train = ids[:train_count]\n",
    "    validation = ids[train_count:]\n",
    "    \n",
    "    print(train.shape)\n",
    "    print(validation.shape)\n",
    "    \n",
    "    partitions = {}\n",
    "    partitions['train'] = train\n",
    "    partitions['validation'] = validation\n",
    "\n",
    "    labels = {}\n",
    "    for index, row in df.iterrows():\n",
    "        label = 'Anomaly'\n",
    "        if (row.hotspot_type == 'Animal'):\n",
    "            label = row.species_id\n",
    "            \n",
    "        labels[str(row.hotspot_id)] = label\n",
    "    \n",
    "    return partitions, labels\n",
    "\n",
    "partitions, labels = GetPartitionsAndLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def LoadImage(id):\n",
    "\n",
    "    # Images are named <originalName>_HOTSPOT_ID.JPG\n",
    "    # ID can be \"1234\" which Pandas converts to \"1234.0\" or \"1234.99\"\n",
    "    if (id.endswith('.0')):\n",
    "        id = id[:-2]\n",
    "\n",
    "    imageMask = \"{}/*HOTSPOT_{}.JPG\".format(imgCropPath, id)\n",
    "    imageFiles = glob(imageMask)\n",
    "    assert len(imageFiles) == 1\n",
    "    \n",
    "    imageFile = imageFiles[0]\n",
    "    image = Image.open(imageFile)\n",
    "    return np.array(image)   \n",
    "\n",
    "LoadImage(partitions['train'][0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly.html\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, list_IDs, labels, batch_size=32, dim=(512,512), n_channels=3,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(list_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.list_IDs))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n",
    "        y = np.empty((self.batch_size), dtype=int)\n",
    "\n",
    "        # Generate data\n",
    "        for i, ID in enumerate(list_IDs_temp):\n",
    "            # Store sample\n",
    "            X[i,] = LoadImage(ID)\n",
    "\n",
    "            # Store class\n",
    "            y[i] = self.labels[ID]\n",
    "\n",
    "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 model layout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ShowLayers(base_model):\n",
    "    for i, layer in enumerate(base_model.layers):\n",
    "        print (i, layer.name, layer.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1 (None, 224, 224, 3)\n",
      "1 block1_conv1 (None, 224, 224, 64)\n",
      "2 block1_conv2 (None, 224, 224, 64)\n",
      "3 block1_pool (None, 112, 112, 64)\n",
      "4 block2_conv1 (None, 112, 112, 128)\n",
      "5 block2_conv2 (None, 112, 112, 128)\n",
      "6 block2_pool (None, 56, 56, 128)\n",
      "7 block3_conv1 (None, 56, 56, 256)\n",
      "8 block3_conv2 (None, 56, 56, 256)\n",
      "9 block3_conv3 (None, 56, 56, 256)\n",
      "10 block3_pool (None, 28, 28, 256)\n",
      "11 block4_conv1 (None, 28, 28, 512)\n",
      "12 block4_conv2 (None, 28, 28, 512)\n",
      "13 block4_conv3 (None, 28, 28, 512)\n",
      "14 block4_pool (None, 14, 14, 512)\n",
      "15 block5_conv1 (None, 14, 14, 512)\n",
      "16 block5_conv2 (None, 14, 14, 512)\n",
      "17 block5_conv3 (None, 14, 14, 512)\n",
      "18 block5_pool (None, 7, 7, 512)\n",
      "19 flatten (None, 25088)\n",
      "20 fc1 (None, 4096)\n",
      "21 fc2 (None, 4096)\n",
      "22 predictions (None, 1000)\n"
     ]
    }
   ],
   "source": [
    "original_model = VGG16(weights=None, include_top=True)\n",
    "ShowLayers(original_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing top and last 3 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_2 (None, 512, 512, 3)\n",
      "1 block1_conv1 (None, 512, 512, 64)\n",
      "2 block1_conv2 (None, 512, 512, 64)\n",
      "3 block1_pool (None, 256, 256, 64)\n",
      "4 block2_conv1 (None, 256, 256, 128)\n",
      "5 block2_conv2 (None, 256, 256, 128)\n",
      "6 block2_pool (None, 128, 128, 128)\n",
      "7 block3_conv1 (None, 128, 128, 256)\n",
      "8 block3_conv2 (None, 128, 128, 256)\n",
      "9 block3_conv3 (None, 128, 128, 256)\n",
      "10 block3_pool (None, 64, 64, 256)\n",
      "11 block4_conv1 (None, 64, 64, 512)\n",
      "12 block4_conv2 (None, 64, 64, 512)\n",
      "13 block4_conv3 (None, 64, 64, 512)\n",
      "14 block4_pool (None, 32, 32, 512)\n",
      "15 block5_conv1 (None, 32, 32, 512)\n",
      "16 block5_conv2 (None, 32, 32, 512)\n",
      "17 block5_conv3 (None, 32, 32, 512)\n",
      "18 block5_pool (None, 16, 16, 512)\n",
      "19 flatten (None, 131072)\n",
      "20 fc1 (None, 4096)\n",
      "21 fc2 (None, 4096)\n",
      "22 predictions (None, 5)\n"
     ]
    }
   ],
   "source": [
    "# Input = 512x512x3\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(512,512,3)) # Will download the 560MB trained model.\n",
    "\n",
    "# Output = ['Anomaly' 'Bearded Seal' 'Ringed Seal' 'Polar Bear' 'UNK Seal']\n",
    "x = base_model.output\n",
    "x = Flatten(name='flatten')(x)\n",
    "x = Dense(4096, name='fc1')(x)\n",
    "x = Dense(4096, name='fc2')(x)\n",
    "predictions = Dense(5, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "ShowLayers(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze VGG layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers: layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (512,512),\n",
    "          'n_channels': 3,\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 5,\n",
    "          'shuffle': False}\n",
    "\n",
    "training_generator = DataGenerator(partition['train'], labels, **params)\n",
    "validation_generator = DataGenerator(partition['validation'], labels, **params)\n",
    "\n",
    "# Train model on dataset\n",
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
